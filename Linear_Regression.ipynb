{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression generally have the form of $Y_{i} = \\theta_{0} + \\theta_{1} x_{1} + \\theta_{2} x_{2} + ...$ <br>\n",
    "There are several ways to find the coefficients of the regression: <br>\n",
    "1. Linear Algebra: $\\hat{\\theta} = (X^{T}X)^{-1}X^{T}Y$ (When X is invertible) <br>\n",
    "2. Gradient Descent: In this case, we need to write out the loss function and try to minimize the loss. <br>\n",
    "$\\hspace{30mm}$ $F(x)$ = Loss Function = SE = $ \\sum^{n}_{i=1} (Y_{i} - \\hat{Y_{i}})^{2}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression():\n",
    "    def __init__(self, alpha = 1e-10 , num_iter = 10000, early_stop = 1e-50, intercept = True, init_weight = None):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "            Some initializations, if neccesary\n",
    "            \n",
    "            attributes: \n",
    "                        alpha: Learning Rate, default 1e-10\n",
    "                        num_iter: Number of Iterations to update coefficient with training data\n",
    "                        early_stop: Constant control early_stop.\n",
    "                        intercept: Bool, If we are going to fit a intercept, default True.\n",
    "                        init_weight: Matrix (n x 1), input init_weight for testing.\n",
    "                        \n",
    "            \n",
    "            TODO: 1. Initialize all variables needed.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model_name = 'Linear Regression'\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.num_iter = num_iter\n",
    "        self.early_stop = early_stop\n",
    "        self.intercept = intercept\n",
    "        self.init_weight = init_weight  ### For testing correctness.\n",
    "        \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "            Save the datasets in our model, and perform gradient descent.\n",
    "            \n",
    "            Parameter:\n",
    "                X_train: Matrix or 2-D array. Input feature matrix.\n",
    "                Y_train: Matrix or 2-D array. Input target value.\n",
    "                \n",
    "                \n",
    "                TODO: 2. If we are going to fit the intercept, add a col with all 1's to the first column. (hint: np.hstack, np.ones)\n",
    "                      3. Initilaize our coef with uniform from [-1, 1] with the number of col in training set.\n",
    "                      4. Call the gradient_descent function to train.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X = np.mat(X_train)\n",
    "        self.y = np.mat(y_train).T\n",
    "        \n",
    "        if self.intercept:\n",
    "            ones = np.ones((X_train.shape[0], 1))\n",
    "            self.X = np.hstack((ones, self.X))\n",
    "        \n",
    "        if self.init_weight is None:\n",
    "            self.init_weight = np.random.uniform(-1, 1, size=(self.X.shape[1], 1))\n",
    "        self.coef = self.init_weight #### Please change this after you get the example right.\n",
    "\n",
    "        self.gradient_descent()\n",
    "        return\n",
    "        \n",
    "    def gradient(self):\n",
    "        \"\"\"\n",
    "            Helper function to calculate the gradient respect to coefficient.\n",
    "            \n",
    "            TODO: 5. Think about the matrix format of the gradient of the loss function.\n",
    "        \"\"\"\n",
    "        predictions = self.X * self.coef\n",
    "        error = predictions - self.y\n",
    "        grad_coef = self.X.T * error / self.X.shape[0]\n",
    "        return grad_coef\n",
    "        \n",
    "    def gradient_descent(self):\n",
    "        \n",
    "        \"\"\"\n",
    "            Training function\n",
    "            \n",
    "            TODO: 6. Calculate the loss with current coefficients.\n",
    "                  7. Update the temp_coef with learning rate and gradient.\n",
    "                  8. Calculate the loss with temp_coef.\n",
    "                  9. Implement the self adeptive learning rate. \n",
    "                      a. If current error is less than previous error, increase learning rate by a factor 1.3. \n",
    "                         And update coef, with temp_coef.\n",
    "                      b. If previous error is less than current error, decrease learning rate by a factor of 0.9.\n",
    "                         Don't update coef.\n",
    "                  10. Add the loss to loss list we create.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.loss = []\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            grad = self.gradient()\n",
    "            temp_coef = self.coef - self.alpha * grad\n",
    "\n",
    "            previous_y_hat = self.X * self.coef\n",
    "            pre_error = np.sum(np.square(self.y - previous_y_hat))\n",
    "\n",
    "            current_y_hat = self.X * temp_coef\n",
    "            current_error = np.sum(np.square(self.y - current_y_hat))\n",
    "\n",
    "            if current_error <= pre_error:\n",
    "                self.coef = temp_coef\n",
    "                self.alpha *= 1.3\n",
    "            else:\n",
    "                self.alpha *= 0.9\n",
    "\n",
    "            if (abs(pre_error - current_error) < self.early_stop) or (abs(abs(pre_error - current_error) / pre_error) < self.early_stop):\n",
    "                return self\n",
    "\n",
    "            # self.loss.append(current_error)\n",
    "            \n",
    "            # if i % 10000 == 0:\n",
    "            #     print('Iteration: ' +  str(i))\n",
    "            #     print('Coef: '+ str(self.coef))\n",
    "            #     print('Loss: ' + str(current_error))            \n",
    "        return self\n",
    "    \n",
    "    def ind_predict(self, x: list):\n",
    "        \"\"\"\n",
    "            Predict the value based on its feature vector x.\n",
    "\n",
    "            Parameter:\n",
    "            x: Matrix, array or list. Input feature point.\n",
    "            \n",
    "            Return:\n",
    "                result: prediction of given data point\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "            TODO: 11. Implement the prediction function\n",
    "        \"\"\"\n",
    "        x = np.mat(x)\n",
    "        result = x * self.coef\n",
    "        return result[0, 0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Parameter:\n",
    "            X: Matrix, array or list. Input feature point.\n",
    "            \n",
    "            Return:\n",
    "                ret: prediction of given data matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "            TODO: 12. Make sure add the 1's column like we did to add intercept.\n",
    "                  13. Revise the following for-loop to call ind_predict to get predictions.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        ret = []\n",
    "        X = np.mat(X)\n",
    "        if self.intercept:\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((ones, X))\n",
    "        for x in X:\n",
    "            ret.append(self.ind_predict(x))\n",
    "        return ret\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normaliz(lst):\n",
    "    \"\"\"\n",
    "    Helper function for normalize for faster training.\n",
    "    \"\"\"\n",
    "    maximum = np.max(lst)\n",
    "    minimum = np.min(lst)\n",
    "\n",
    "    return (lst - minimum) / (maximum - minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We generate some easy data for testing. We should fit a line with, $Y = 30 * X + 20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(np.mat(np.arange(1, 1000, 5)).T)\n",
    "y = np.array((30 * X)).flatten() +  20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do NOT modify the following line, just run it when you are done.  You can also try different initialization, you will notice different coef at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = Linear_Regression(alpha = 1, num_iter = 10000000, init_weight= np.mat([15,25]).T)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  As the number of iteration increase, you should notice the coeficient converges to [20, 30]. \n",
    "#### It maybe very slow update. Feel free to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[19.93498082],\n",
       "        [30.00009741]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   49.93507823,   199.9355653 ,   349.93605236,   499.93653942,\n",
       "         649.93702649,   799.93751355,   949.93800061,  1099.93848768,\n",
       "        1249.93897474,  1399.9394618 ,  1549.93994887,  1699.94043593,\n",
       "        1849.94092299,  1999.94141006,  2149.94189712,  2299.94238418,\n",
       "        2449.94287125,  2599.94335831,  2749.94384537,  2899.94433244,\n",
       "        3049.9448195 ,  3199.94530656,  3349.94579363,  3499.94628069,\n",
       "        3649.94676775,  3799.94725482,  3949.94774188,  4099.94822894,\n",
       "        4249.94871601,  4399.94920307,  4549.94969013,  4699.9501772 ,\n",
       "        4849.95066426,  4999.95115132,  5149.95163839,  5299.95212545,\n",
       "        5449.95261251,  5599.95309958,  5749.95358664,  5899.9540737 ,\n",
       "        6049.95456077,  6199.95504783,  6349.95553489,  6499.95602195,\n",
       "        6649.95650902,  6799.95699608,  6949.95748314,  7099.95797021,\n",
       "        7249.95845727,  7399.95894433,  7549.9594314 ,  7699.95991846,\n",
       "        7849.96040552,  7999.96089259,  8149.96137965,  8299.96186671,\n",
       "        8449.96235378,  8599.96284084,  8749.9633279 ,  8899.96381497,\n",
       "        9049.96430203,  9199.96478909,  9349.96527616,  9499.96576322,\n",
       "        9649.96625028,  9799.96673735,  9949.96722441, 10099.96771147,\n",
       "       10249.96819854, 10399.9686856 , 10549.96917266, 10699.96965973,\n",
       "       10849.97014679, 10999.97063385, 11149.97112092, 11299.97160798,\n",
       "       11449.97209504, 11599.97258211, 11749.97306917, 11899.97355623,\n",
       "       12049.9740433 , 12199.97453036, 12349.97501742, 12499.97550449,\n",
       "       12649.97599155, 12799.97647861, 12949.97696568, 13099.97745274,\n",
       "       13249.9779398 , 13399.97842687, 13549.97891393, 13699.97940099,\n",
       "       13849.97988806, 13999.98037512, 14149.98086218, 14299.98134924,\n",
       "       14449.98183631, 14599.98232337, 14749.98281043, 14899.9832975 ,\n",
       "       15049.98378456, 15199.98427162, 15349.98475869, 15499.98524575,\n",
       "       15649.98573281, 15799.98621988, 15949.98670694, 16099.987194  ,\n",
       "       16249.98768107, 16399.98816813, 16549.98865519, 16699.98914226,\n",
       "       16849.98962932, 16999.99011638, 17149.99060345, 17299.99109051,\n",
       "       17449.99157757, 17599.99206464, 17749.9925517 , 17899.99303876,\n",
       "       18049.99352583, 18199.99401289, 18349.99449995, 18499.99498702,\n",
       "       18649.99547408, 18799.99596114, 18949.99644821, 19099.99693527,\n",
       "       19249.99742233, 19399.9979094 , 19549.99839646, 19699.99888352,\n",
       "       19849.99937059, 19999.99985765, 20150.00034471, 20300.00083178,\n",
       "       20450.00131884, 20600.0018059 , 20750.00229297, 20900.00278003,\n",
       "       21050.00326709, 21200.00375416, 21350.00424122, 21500.00472828,\n",
       "       21650.00521534, 21800.00570241, 21950.00618947, 22100.00667653,\n",
       "       22250.0071636 , 22400.00765066, 22550.00813772, 22700.00862479,\n",
       "       22850.00911185, 23000.00959891, 23150.01008598, 23300.01057304,\n",
       "       23450.0110601 , 23600.01154717, 23750.01203423, 23900.01252129,\n",
       "       24050.01300836, 24200.01349542, 24350.01398248, 24500.01446955,\n",
       "       24650.01495661, 24800.01544367, 24950.01593074, 25100.0164178 ,\n",
       "       25250.01690486, 25400.01739193, 25550.01787899, 25700.01836605,\n",
       "       25850.01885312, 26000.01934018, 26150.01982724, 26300.02031431,\n",
       "       26450.02080137, 26600.02128843, 26750.0217755 , 26900.02226256,\n",
       "       27050.02274962, 27200.02323669, 27350.02372375, 27500.02421081,\n",
       "       27650.02469788, 27800.02518494, 27950.025672  , 28100.02615907,\n",
       "       28250.02664613, 28400.02713319, 28550.02762026, 28700.02810732,\n",
       "       28850.02859438, 29000.02908144, 29150.02956851, 29300.03005557,\n",
       "       29450.03054263, 29600.0310297 , 29750.03151676, 29900.03200382])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = min_max_normaliz(X)\n",
    "y = min_max_normaliz(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please try to normalize the X and fit again with normalized X. You should find something interesting. Also think about what you should do for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Linear_Regression(alpha = 1, num_iter = 5000000)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[6.96883989e-25],\n",
       "        [1.00000000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.96883989e-25, 5.02512563e-03, 1.00502513e-02, 1.50753769e-02,\n",
       "       2.01005025e-02, 2.51256281e-02, 3.01507538e-02, 3.51758794e-02,\n",
       "       4.02010050e-02, 4.52261307e-02, 5.02512563e-02, 5.52763819e-02,\n",
       "       6.03015075e-02, 6.53266332e-02, 7.03517588e-02, 7.53768844e-02,\n",
       "       8.04020101e-02, 8.54271357e-02, 9.04522613e-02, 9.54773869e-02,\n",
       "       1.00502513e-01, 1.05527638e-01, 1.10552764e-01, 1.15577889e-01,\n",
       "       1.20603015e-01, 1.25628141e-01, 1.30653266e-01, 1.35678392e-01,\n",
       "       1.40703518e-01, 1.45728643e-01, 1.50753769e-01, 1.55778894e-01,\n",
       "       1.60804020e-01, 1.65829146e-01, 1.70854271e-01, 1.75879397e-01,\n",
       "       1.80904523e-01, 1.85929648e-01, 1.90954774e-01, 1.95979899e-01,\n",
       "       2.01005025e-01, 2.06030151e-01, 2.11055276e-01, 2.16080402e-01,\n",
       "       2.21105528e-01, 2.26130653e-01, 2.31155779e-01, 2.36180905e-01,\n",
       "       2.41206030e-01, 2.46231156e-01, 2.51256281e-01, 2.56281407e-01,\n",
       "       2.61306533e-01, 2.66331658e-01, 2.71356784e-01, 2.76381910e-01,\n",
       "       2.81407035e-01, 2.86432161e-01, 2.91457286e-01, 2.96482412e-01,\n",
       "       3.01507538e-01, 3.06532663e-01, 3.11557789e-01, 3.16582915e-01,\n",
       "       3.21608040e-01, 3.26633166e-01, 3.31658291e-01, 3.36683417e-01,\n",
       "       3.41708543e-01, 3.46733668e-01, 3.51758794e-01, 3.56783920e-01,\n",
       "       3.61809045e-01, 3.66834171e-01, 3.71859296e-01, 3.76884422e-01,\n",
       "       3.81909548e-01, 3.86934673e-01, 3.91959799e-01, 3.96984925e-01,\n",
       "       4.02010050e-01, 4.07035176e-01, 4.12060302e-01, 4.17085427e-01,\n",
       "       4.22110553e-01, 4.27135678e-01, 4.32160804e-01, 4.37185930e-01,\n",
       "       4.42211055e-01, 4.47236181e-01, 4.52261307e-01, 4.57286432e-01,\n",
       "       4.62311558e-01, 4.67336683e-01, 4.72361809e-01, 4.77386935e-01,\n",
       "       4.82412060e-01, 4.87437186e-01, 4.92462312e-01, 4.97487437e-01,\n",
       "       5.02512563e-01, 5.07537688e-01, 5.12562814e-01, 5.17587940e-01,\n",
       "       5.22613065e-01, 5.27638191e-01, 5.32663317e-01, 5.37688442e-01,\n",
       "       5.42713568e-01, 5.47738693e-01, 5.52763819e-01, 5.57788945e-01,\n",
       "       5.62814070e-01, 5.67839196e-01, 5.72864322e-01, 5.77889447e-01,\n",
       "       5.82914573e-01, 5.87939698e-01, 5.92964824e-01, 5.97989950e-01,\n",
       "       6.03015075e-01, 6.08040201e-01, 6.13065327e-01, 6.18090452e-01,\n",
       "       6.23115578e-01, 6.28140704e-01, 6.33165829e-01, 6.38190955e-01,\n",
       "       6.43216080e-01, 6.48241206e-01, 6.53266332e-01, 6.58291457e-01,\n",
       "       6.63316583e-01, 6.68341709e-01, 6.73366834e-01, 6.78391960e-01,\n",
       "       6.83417085e-01, 6.88442211e-01, 6.93467337e-01, 6.98492462e-01,\n",
       "       7.03517588e-01, 7.08542714e-01, 7.13567839e-01, 7.18592965e-01,\n",
       "       7.23618090e-01, 7.28643216e-01, 7.33668342e-01, 7.38693467e-01,\n",
       "       7.43718593e-01, 7.48743719e-01, 7.53768844e-01, 7.58793970e-01,\n",
       "       7.63819095e-01, 7.68844221e-01, 7.73869347e-01, 7.78894472e-01,\n",
       "       7.83919598e-01, 7.88944724e-01, 7.93969849e-01, 7.98994975e-01,\n",
       "       8.04020101e-01, 8.09045226e-01, 8.14070352e-01, 8.19095477e-01,\n",
       "       8.24120603e-01, 8.29145729e-01, 8.34170854e-01, 8.39195980e-01,\n",
       "       8.44221106e-01, 8.49246231e-01, 8.54271357e-01, 8.59296482e-01,\n",
       "       8.64321608e-01, 8.69346734e-01, 8.74371859e-01, 8.79396985e-01,\n",
       "       8.84422111e-01, 8.89447236e-01, 8.94472362e-01, 8.99497487e-01,\n",
       "       9.04522613e-01, 9.09547739e-01, 9.14572864e-01, 9.19597990e-01,\n",
       "       9.24623116e-01, 9.29648241e-01, 9.34673367e-01, 9.39698492e-01,\n",
       "       9.44723618e-01, 9.49748744e-01, 9.54773869e-01, 9.59798995e-01,\n",
       "       9.64824121e-01, 9.69849246e-01, 9.74874372e-01, 9.79899497e-01,\n",
       "       9.84924623e-01, 9.89949749e-01, 9.94974874e-01, 1.00000000e+00])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.00502513],\n",
       "       [0.01005025],\n",
       "       [0.01507538],\n",
       "       [0.0201005 ],\n",
       "       [0.02512563],\n",
       "       [0.03015075],\n",
       "       [0.03517588],\n",
       "       [0.04020101],\n",
       "       [0.04522613],\n",
       "       [0.05025126],\n",
       "       [0.05527638],\n",
       "       [0.06030151],\n",
       "       [0.06532663],\n",
       "       [0.07035176],\n",
       "       [0.07537688],\n",
       "       [0.08040201],\n",
       "       [0.08542714],\n",
       "       [0.09045226],\n",
       "       [0.09547739],\n",
       "       [0.10050251],\n",
       "       [0.10552764],\n",
       "       [0.11055276],\n",
       "       [0.11557789],\n",
       "       [0.12060302],\n",
       "       [0.12562814],\n",
       "       [0.13065327],\n",
       "       [0.13567839],\n",
       "       [0.14070352],\n",
       "       [0.14572864],\n",
       "       [0.15075377],\n",
       "       [0.15577889],\n",
       "       [0.16080402],\n",
       "       [0.16582915],\n",
       "       [0.17085427],\n",
       "       [0.1758794 ],\n",
       "       [0.18090452],\n",
       "       [0.18592965],\n",
       "       [0.19095477],\n",
       "       [0.1959799 ],\n",
       "       [0.20100503],\n",
       "       [0.20603015],\n",
       "       [0.21105528],\n",
       "       [0.2160804 ],\n",
       "       [0.22110553],\n",
       "       [0.22613065],\n",
       "       [0.23115578],\n",
       "       [0.2361809 ],\n",
       "       [0.24120603],\n",
       "       [0.24623116],\n",
       "       [0.25125628],\n",
       "       [0.25628141],\n",
       "       [0.26130653],\n",
       "       [0.26633166],\n",
       "       [0.27135678],\n",
       "       [0.27638191],\n",
       "       [0.28140704],\n",
       "       [0.28643216],\n",
       "       [0.29145729],\n",
       "       [0.29648241],\n",
       "       [0.30150754],\n",
       "       [0.30653266],\n",
       "       [0.31155779],\n",
       "       [0.31658291],\n",
       "       [0.32160804],\n",
       "       [0.32663317],\n",
       "       [0.33165829],\n",
       "       [0.33668342],\n",
       "       [0.34170854],\n",
       "       [0.34673367],\n",
       "       [0.35175879],\n",
       "       [0.35678392],\n",
       "       [0.36180905],\n",
       "       [0.36683417],\n",
       "       [0.3718593 ],\n",
       "       [0.37688442],\n",
       "       [0.38190955],\n",
       "       [0.38693467],\n",
       "       [0.3919598 ],\n",
       "       [0.39698492],\n",
       "       [0.40201005],\n",
       "       [0.40703518],\n",
       "       [0.4120603 ],\n",
       "       [0.41708543],\n",
       "       [0.42211055],\n",
       "       [0.42713568],\n",
       "       [0.4321608 ],\n",
       "       [0.43718593],\n",
       "       [0.44221106],\n",
       "       [0.44723618],\n",
       "       [0.45226131],\n",
       "       [0.45728643],\n",
       "       [0.46231156],\n",
       "       [0.46733668],\n",
       "       [0.47236181],\n",
       "       [0.47738693],\n",
       "       [0.48241206],\n",
       "       [0.48743719],\n",
       "       [0.49246231],\n",
       "       [0.49748744],\n",
       "       [0.50251256],\n",
       "       [0.50753769],\n",
       "       [0.51256281],\n",
       "       [0.51758794],\n",
       "       [0.52261307],\n",
       "       [0.52763819],\n",
       "       [0.53266332],\n",
       "       [0.53768844],\n",
       "       [0.54271357],\n",
       "       [0.54773869],\n",
       "       [0.55276382],\n",
       "       [0.55778894],\n",
       "       [0.56281407],\n",
       "       [0.5678392 ],\n",
       "       [0.57286432],\n",
       "       [0.57788945],\n",
       "       [0.58291457],\n",
       "       [0.5879397 ],\n",
       "       [0.59296482],\n",
       "       [0.59798995],\n",
       "       [0.60301508],\n",
       "       [0.6080402 ],\n",
       "       [0.61306533],\n",
       "       [0.61809045],\n",
       "       [0.62311558],\n",
       "       [0.6281407 ],\n",
       "       [0.63316583],\n",
       "       [0.63819095],\n",
       "       [0.64321608],\n",
       "       [0.64824121],\n",
       "       [0.65326633],\n",
       "       [0.65829146],\n",
       "       [0.66331658],\n",
       "       [0.66834171],\n",
       "       [0.67336683],\n",
       "       [0.67839196],\n",
       "       [0.68341709],\n",
       "       [0.68844221],\n",
       "       [0.69346734],\n",
       "       [0.69849246],\n",
       "       [0.70351759],\n",
       "       [0.70854271],\n",
       "       [0.71356784],\n",
       "       [0.71859296],\n",
       "       [0.72361809],\n",
       "       [0.72864322],\n",
       "       [0.73366834],\n",
       "       [0.73869347],\n",
       "       [0.74371859],\n",
       "       [0.74874372],\n",
       "       [0.75376884],\n",
       "       [0.75879397],\n",
       "       [0.7638191 ],\n",
       "       [0.76884422],\n",
       "       [0.77386935],\n",
       "       [0.77889447],\n",
       "       [0.7839196 ],\n",
       "       [0.78894472],\n",
       "       [0.79396985],\n",
       "       [0.79899497],\n",
       "       [0.8040201 ],\n",
       "       [0.80904523],\n",
       "       [0.81407035],\n",
       "       [0.81909548],\n",
       "       [0.8241206 ],\n",
       "       [0.82914573],\n",
       "       [0.83417085],\n",
       "       [0.83919598],\n",
       "       [0.84422111],\n",
       "       [0.84924623],\n",
       "       [0.85427136],\n",
       "       [0.85929648],\n",
       "       [0.86432161],\n",
       "       [0.86934673],\n",
       "       [0.87437186],\n",
       "       [0.87939698],\n",
       "       [0.88442211],\n",
       "       [0.88944724],\n",
       "       [0.89447236],\n",
       "       [0.89949749],\n",
       "       [0.90452261],\n",
       "       [0.90954774],\n",
       "       [0.91457286],\n",
       "       [0.91959799],\n",
       "       [0.92462312],\n",
       "       [0.92964824],\n",
       "       [0.93467337],\n",
       "       [0.93969849],\n",
       "       [0.94472362],\n",
       "       [0.94974874],\n",
       "       [0.95477387],\n",
       "       [0.95979899],\n",
       "       [0.96482412],\n",
       "       [0.96984925],\n",
       "       [0.97487437],\n",
       "       [0.9798995 ],\n",
       "       [0.98492462],\n",
       "       [0.98994975],\n",
       "       [0.99497487],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You can also try this with the wine dataset we use in HW1. Try fit this function to that dataset with same features. If you look closely to the updates of coefficients. What do you find? This could be mentioned in your report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')\n",
    "X = wine[['density','alcohol']]\n",
    "y = wine.quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LinearRegression.__init__() got an unexpected keyword argument 'num_iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m lr\u001b[38;5;241m.\u001b[39mfit(X,y)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m## Squared Error with sklearn.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: LinearRegression.__init__() got an unexpected keyword argument 'num_iter'"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(num_iter = 5000000)\n",
    "lr.fit(X,y)\n",
    "## Squared Error with sklearn.\n",
    "sum((lr.predict(X) - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34.82170159,  0.39144139])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will notice different coefficients, but the loss is very close to each other like 805. In your report, briefly discuss this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Linear_Regression(alpha = 1, num_iter = 5000000)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805.2567476112227"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((clf.predict(X) - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.25254598],\n",
       "        [ 2.11924863],\n",
       "        [ 0.3622979 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
